---
title: "Эконометрика"
author: "Демешев Борис Борисович"
output:
  pdf_document: default
  html_document: default
  word_document: default
header-includes:
- \usepackage[english, russian]{babel}
- \usepackage{amsmath,calc}
---
```{r setup, include = F}
library(knitr) # нужен для ф-ии kable
```

### 1.1.1. Суть метода наименьших квадратов
#### Общие сведения
Эконометрика отвечает на два вопроса:

* Как переменная $x$ влияет на переменную $y$
* И как спрогнозировать переменную $y$

Ответы на эти два вопроса мы получаем с помощью моделей.

**Модель** - некая формула, которая связывает объясняемую переменную $y$ и объясняющую
переменную $x$ (одну или несколько).

Например модель может иметь вид: $y_i=\beta_1 +\beta_2x_i+\varepsilon_i$

#### Типы данных
Прежде чем говорить об оценивании моделей, выборе модели, надо начать с данных.
Без данных моделей не бывает. В эконометрике встречается много разных типов данных, но
пожалуй самые простые базовые типы данных, это:

* **Временные ряды** - есть некий индекс времени, например год и показатели, меняющиеся во времени;
т.е. это несколько показателей на разные моменты времени; время, как правило регулярны (год, месяц и т.д.).
* **Перекрестные данные** - это когда есть несколько объетов на один момент времени и для каждого объекта есть свой набор показателей (читай сводная таблица).
* **Панельные данные** - это сочетание первых двух, например несколько перекрестных таблиц упорядоченных
во временном ряду.

#### Данные - обозначения
* одна зависимая (**объясняемая**) переменная будет обозначаться $y$; это так переменная которую мы
хотим предсказать, или понять от чего она зависит.
* несколько регрессоров (**объясняющих** переменных) будут обозначаться $x,z,\dots$
* по каждой переменной будет $n$ наблюдений: $y_1, y_2, y_3, \dots, y_n$

#### Данные - пример
Исторические данные 1920-х годов по автомобилям. Были измерены скорости автомобилей $y_i$ 
и длины тормозного пути $x_i$. 

При работе с данными **ВСЕГДА** отображайте их. Никакой эконометрический анализ не заменит простой график. Нарисовав график мы увидим ожидаемую взаимосвязь - чем больше скорость, тем больше длина 
тормозного пути, в среднем. 

#### Модель
В таком случае предположим, что модель имеет простую линейную форму:
$$y_i=\beta_1 +\beta_2x_i+\varepsilon_i$$
У нас есть:

* наблюдаемые переменные: $y, x$
* неизвестные параметры: $\beta_1, \beta_2$
* случайная составляющая, ошибка: $\varepsilon$

План действия:

* придумать адекватную модель - по построенному графику нам вполне подходит линейная модель
* получить оценкин еизвестных параметров: $\hat{\beta_1}, \hat{\beta_2}$; 
другими словами нам нужен некий метод, для получения оценок.
* затем, если мы хотим прогнозировать или интерпретировать наши данные, то можно ипользовать полученные
оценки: $\hat{y_i}=\hat{\beta_1} + \hat{\beta_2}x_i$

**М**етод **Н**аименьших **К**вадратов - это самый простой и самый популярный метод оценки $\hat{\beta_1}, \hat{\beta_2}$.

Суть метода - получить оценки неизвестных параметров, исходя из реальных данных.

Если мы придумали какие-то оценки $\hat{\beta_1}, \hat{\beta_2}$, то 
естественно возникает такое понятие как **Ошибка прогноза**:
$$\hat{\varepsilon_i}=y_i - \hat{y_i}$$
Отсюда возникает суммарная ошибка прогноза:
$$Q(\hat{\beta_1}, \hat{\beta_2})=\sum_{i=1}^n{\hat{\varepsilon_i}^2}=\sum_{i=1}^n{(y_i - \hat{y_i})^2}$$
Чтобы ошибки не компенсировали друг друга (одна в плюс, другая в минус) мы возведем их в квадрат и
посчитаем сумму квадратов ошибок прогноза.

#### Суть МНК
Возьмите в качестве оценок такие $\hat{\beta_1}, \hat{\beta_2}$, при которых сумма квадратов ошибок
прогноза $Q$ минимальна.

#### Пример с машинами
Прогнав данные через **R**, мы получили:
$$\hat{\beta_1}=-5.3,\quad \hat{\beta_2}=0.7$$
$$\hat{y_i}=-5.3 +0.7x_i$$

### 1.1.2. Пример. Регрессия на константу (у доски)
Простой набор данных: есть три человека, их рост и вес.

<!-- $$ -->
<!-- \begin{table} -->
<!-- {rcc} -->
<!--  & $y_i$ (вес, кг) & $x_i$ (рост, см) \\ -->
<!--  Вася & 60 & 170 \\ -->
<!--  Коля & 70 & 170 \\ -->
<!--  Петя & 80 & 181 -->
<!-- \end{table} -->
<!-- $$ -->

```{r, echo = F}
kable(data.frame(c("Вася", "Коля", "Петя"), 
                 c(60,70,80),
                 c(170, 170, 181)),
      col.names = c("", "рост, см", "вес, кг"))
```

Модель №1: $y_i=\beta +\varepsilon_i$

Модель №2: $y_i=\beta_1 +\beta_2 x_i + \varepsilon_i$

Нам необходимо получить $\beta;\beta_1, \beta_2$ с помощью **МНК**.

Первая модель предполгаает, что вес не зависит от роста, т.е. вес человека это некая
константа плюс случайная составляющая для каждого человека.

Вторая модель предполагает, что вес зависить от роста линейно.

Нам нужно оценить эти две модели.

Что делает МНК? Он минимизирует RSS: $min\sum{(y_i-\hat{y_i})^2}$, подбирая 
коэффициенты $\hat\beta$.

#### Модель №1
Вместо настоящего коэф. пишем его оценку, 
а ошибка $\varepsilon$ вообще не прогнозируема, пишем вместо неё ноль.
$$\hat{y_i}=\hat{\beta}$$

Тогда получаем:
$$
\sum_{i=1}^3{(y_i-\hat{y_i})^2}=\sum_{i=1}^3{(y_i-\hat{\beta})^2}=\\
=(y_1-\hat{\beta})^2-(y_2-\hat{\beta})^2-(y_3-\hat{\beta})^2=Q(\hat{\beta})
$$
Давайте раскроем скобки под суммой:
$$
\sum_{i=1}^3{(y_i^2-2\hat{\beta}y_i + \hat{\beta^2})}=\\
=\sum_{i=1}^3{y_i^2}-\sum_{i=1}^3{2\hat{\beta}y_i}+\sum_{i=1}^3{\hat{\beta^2}}
$$
Заметим, что последняя сумма не зависит от $i$;
$$
\sum_{i=1}^3{\hat{\beta^2}}=\hat{\beta^2}+\hat{\beta^2}+\hat{\beta^2}=3\hat{\beta^2}
\qquad (n)\hat{\beta^2} 
$$

Во втором слагаемом выносим $2\hat{\beta}$ за знак суммы:
$$
\sum_{i=1}^3{2\hat{\beta}y_i}=2\hat{\beta} \sum_{i=1}^3{y_i}
$$
Выведем универсальную формулу для первой модели (для кол-ва переменных $n$):
$$
Q(\hat{y_i})=\sum{y_i^2}-2\hat{\beta}\sum{y_i}+n\hat{\beta^2}
$$

И возьмем от нее производную для поиска экстремума:
$$
Q'(\hat{\beta})=-2\sum{y_i}+2n\hat{\beta}
$$

Заметим, что наша функция $Q(\hat{\beta})$ является параболой с ветвями направленными вверх,
т.к. коэф $n$ при $\hat{\beta}$ является положительным (это кол-во наблюдений).

Приравняем производную $Q(\hat{\beta})$ к нулю:

$$
Q'(\hat{\beta})=-2\sum{y_i}+2n\hat{\beta}=0
$$

Выражаем отсюда $\hat{\beta}$:
$$
\hat{\beta}=\frac{\sum{y_i}}{n}=\bar{y}
$$
Получем среднее значение:
$$
M1: \quad \hat{\beta}=\frac{60+70+80}{3}=70
$$

### 1.1.3. - 1.1.4. Пример 2. Парная регрессия.
Перейдем к оценке второй более сложной модели Методом Наименьших квадратов.

Для начала нам понадобится некое вспомогательное наблюдение, чтобы легче было 
следовать за выкладками. 

Заметим, что если посчитать среднее:
$$
\frac{\sum_{i=1}^n{x_i}}{n}=\bar{x}
$$
Из этого следует:
$$
\sum_{i=1}^n{x_i}=n\cdot\bar{x}=\sum_{i=1}^n{\bar{x}}
$$
Эти две суммы можно записать еще вот так:
$$
\sum_{i=1}^n(x_i-\bar{x})=0
$$
Вооружившись таким знанием мы готовы переходить к оценке второй модели:
$$
RSS=\sum_{i=1}^n(y_i-\bar{y})^2
$$
Для второй модели:
$$
M2: \quad \hat{y_i}=\hat\beta_1 + \hat\beta_2 x_i
$$
Подставляем $\hat y$ в формулу с квадратом ошибок:
$$
RSS=\sum_{i=1}^n(y_i-\hat\beta_1 -\hat\beta_2x_i)^2=Q(\hat\beta_1,\hat\beta_2)
$$
Минимизируем эту функцию $Q(\hat\beta_1,\hat\beta_2)$ взяв производную:
$$
\left\{
  \begin{aligned}
    \frac{\partial Q}{\partial \hat\beta_1} &=
    \sum 2 \cdot (y_i-\hat\beta_1 -\hat\beta_2x_i) \cdot (-1) = 0 \\
    \frac{\partial Q}{\partial \hat\beta_2} &=
    \sum 2 \cdot 
      \underbrace{(y_i-\hat\beta_1 -\hat\beta_2x_i)}_{\Large\hat\varepsilon_i} 
    \cdot (-x_i) = 0
  \end{aligned}
\right.
$$
Это условия первого порядка и из этой мимтемы уравнений у нас $y_i$ даны, $x_i$ даны, 
в этих двух уравнениях неизвестны только $\hat\beta_1, \hat\beta_2$.
$$
\left\{
  \begin{aligned}
    &\sum_{i=1}^n \hat\varepsilon_i \cdot 1=0 \\
    &\sum_{i=1}^n \hat\varepsilon_i \cdot x_i=0
  \end{aligned}
\right.
$$
Решаем первую систему уравнений; сократим на $-2$ и раскроем скобки:
$$
\left\{
  \begin{aligned}
    & \sum y_i - \sum \hat\beta_1 - \sum \hat\beta_2 x_i=0 \\
    & \sum y_i \cdot x_i - \sum \hat\beta_1 \cdot x_i - \sum \hat\beta_2 \cdot x_i^2 =0
  \end{aligned}
\right.
$$
Вынесем константы за знак суммы и поделим на $n$:
$$
\left\{
  \begin{aligned}
    &\sum y_i - n\cdot\hat\beta_1 - \hat\beta_2\cdot \sum x_i = 0 \\
    &\sum y_i \cdot x_i - \hat\beta_1 \cdot x_i - \hat\beta_2\cdot \sum x_i^2 = 0
  \end{aligned}
\right.
$$
Делим первое уравнение в системе на $n$ и получаем:
$$
\bar y - \hat\beta_1 - \hat\beta_2 \cdot \bar x=0 \\
\text {или} \\
\bar y = \hat\beta_1 + \hat\beta_2 \cdot \bar x
$$

Выражаем $\hat\beta_1=\bar{y} -\hat\beta_2 \cdot \hat{x}$ 
и подставляем во второе уравнение:
$$
\sum{y_i x_i}-(\bar{y}- \hat\beta_2 \bar{x})\cdot\sum{x_i}-\hat\beta_2\cdot{x_i}^2=0 \\
\sum{y_i x_i}-\bar{y}\cdot\sum{x_i}+\hat\beta_2\cdot\bar{x}\cdot\sum{x_i}-
  \hat\beta_2\cdot{x_i}^2=0 \\
\hat\beta_2(\bar{x}\sum{x_i}-\sum{x_i^2})=\bar{y}\cdot\sum{x_i}-\sum{y_i x_i} \\
\hat\beta_2=\frac{\bar{y}\cdot\sum{x_i}-\sum{y_i x_i}}{\bar{x}\sum{x_i}-\sum{x_i^2}}
$$

Приведем конечную формулу к более классическому виду; помножим числитель и знаменатель на $-1$;
так же внесем под знак суммы $\bar{y}, \bar{x}$, т.к. это константы:
$$
\hat\beta_2=\frac{\bar{y}\cdot\sum{x_i}-\sum{y_i x_i}}{\bar{x}\sum{x_i}-\sum{x_i^2}} =
\quad \times (-1) \\
=\frac{\sum{y_ix_i}-\sum{\bar{y}x_i}}{\sum{x_i^2}-\sum{\bar{x}x_i}}=
\frac{\sum(y_i-\bar{y})x_i}{\sum(x_i-\bar{x})x_i}
$$
Вспомним, что $\sum(x_i-\bar{x})=0$; соответственно, если домножить сумму  
на любой показатель (допустим на $\bar{x}$), то она тоже будет равняться нулю: 
$\bar{x}\sum(x_i-\bar{x})=0 \Rightarrow \sum\bar{x}(x_i-\bar{x})=0$

Вычтем подобные "нули" из числителя и знаменателя и далее внесем все под один знак суммы:
$$
\hat\beta_2=
  \frac
  {\sum(y_i-\bar{y})x_i-\overbrace{\sum\bar{x}(y_i-\bar{y})}^{0}}
  {\sum(x_i-\bar{x})x_i-\underbrace{\sum\bar{x}(x_i-\bar{x})}_{0}}= \\
=\frac{\sum(y_i-\bar{y})\cdot(x_i-\bar{x})}{\sum(x_i-\bar{x})^2}
$$
Эта форма записи xороша тем, что везде фигурирует отклонение наблюдений от среднего значения.


Если подставить все $x_i$ и $y_i$, то получим следующте оценки:
$$
\hat\beta_2 = 1,36 \qquad \hat\beta_1 = -166,8
$$
Таким образом мы получили две формулы для $\hat\beta_1, \hat\beta_2$

### 1.1.5 МНК на графике. Случай множеста регрессоров.
Применив МНК к двум простым моделям мы получили следующие результаты:

В модели $y_i=\beta+\varepsilon_i$:
$$\hat\beta=\bar{y}$$
В линейной модели $y_i=\beta_1+\beta_2x_i+\varepsilon_i$:
$$
\hat\beta_1=\bar{y} -\hat\beta_2 \cdot \hat{x}
\qquad
\hat\beta_2=\frac{\sum(y_i-\bar{y})\cdot(x_i-\bar{x})}{\sum(x_i-\bar{x})^2}
$$
Первое уравнение означает, что линия регресии $\hat\beta_1=\bar{y} -\hat\beta_2 \cdot \hat{x}$
проходит через точку $(\bar{x}, \bar{y})$ 

#### Подведем итог используемой терминологи.
$y_i$ - зависимая, объясняемая переменная;

$x_i$ - регрессор, объясняющая переменная;

$\varepsilon_i$ - ошибка, ошибка модели, случайная составляющая, 
непредсказумая/немоделируемая величина;

$\hat y_i$ - прогноз, прогнозное значение;

$\hat{\varepsilon_i}=y_i-\hat{y_i}$ - остаток, ошибка прогноза;

$RSS = \sum_{i=1}^n{\hat\varepsilon_i^2}$ - сумма квадратов остатков или 
сумма квадратов ошибок прогноза (Residual Sum of Squares);

#### <span style="color:red">Далее следует отрывок с графическим пояснением МНК - НАРИСОВАТЬ!!!!!!!!!!!!!!!!</span>

#### Много объясняющих переменных.
Вринципе, дважцать случай с двадцатью объясняющими переменными концептуально
ничем не отличается от случая с двумя объясняющими переменными.

$$
y_i=\beta_1+\beta_2 x_i + \beta_2 z_i +\varepsilon_i
$$
Наша задача выписать систему уравнений для оценок $\hat\beta_1,\hat\beta_2,\hat\beta_3$.

У нас имеется модель:
$$
y_i=\beta_1+\beta_2 x_i + \beta_2 z_i +\varepsilon_i
$$
И прогноз для нее:
$$
\hat{y_i}=\hat\beta_1+\hat\beta_2 \hat{x_i} + \hat\beta_2 \hat{z_i}
$$
ошибка прогноза:
$$
\hat\varepsilon_i=y_i-\hat{y_i}
$$
И Метод Наименьших Квадратов минимизирует ошибку прогноза:
$$
МНК: \min \sum \hat\varepsilon_i^2=Q(\hat\beta_1,\hat\beta_2,\hat\beta_3)
$$
Заметим, что $y_i$ не зависит от $\hat\beta_1,\hat\beta_2,\hat\beta_3$, от них зависят только прогнозы.
Тогда возьмем производные от $\hat\varepsilon_i$ по $\hat\beta_1,\hat\beta_2,\hat\beta_3$:
$$
\left\{
  \begin{aligned}
    \frac{\partial\hat\varepsilon}{\partial\hat\beta_1} =-1
    \quad
    \frac{\partial\hat\varepsilon}{\partial\hat\beta_2} =-x_i
    \quad
    \frac{\partial\hat\varepsilon}{\partial\hat\beta_3} =-z_i
  \end{aligned}
\right.
$$
Теперь можно выписать условия первого порядка:
$$
\left\{
\begin{aligned}
  \frac{\partial Q}{\partial\hat\beta_1}=0 
  \quad
  \frac{\partial Q}{\partial\hat\beta_2}=0
  \quad
  \frac{\partial Q}{\partial\hat\beta_3}=0
\end{aligned}
\right.
$$
Возьмем производную от суммы квадратов ошибок:
$$
\left\{
\begin{aligned}
  & \sum 2 \cdot \hat\varepsilon_i \cdot (-1)=0  \\
  & \sum 2 \cdot \hat\varepsilon_i \cdot (-x_i)=0  \\
  & \sum 2 \cdot \hat\varepsilon_i \cdot (-z_i)=0  
\end{aligned}
\right.
$$
$$
\left\{
\begin{aligned}
  & \sum \hat\varepsilon_i \cdot 1=0  \\
  & \sum \hat\varepsilon_i \cdot x_i=0  \\
  & \sum \hat\varepsilon_i \cdot z_i=0  
\end{aligned}
\right.
$$

Мы получили условия первого опрядка и из этой системы можно найти три неизвестных
$\hat\beta_1,\hat\beta_2,\hat\beta_3$.

### 1.1.6 Ликбез по линейной алгебре 
Среди прочих показателй, рассчитываемых в МНК, фигурирует три суммы квадратов. Эти суммы квадратов меряют изменчивость нескольких показателей:

* $RSS=\sum\hat\varepsilon_i^2$ - сумма квадратов остатков (Residual Sum of Squares)
* $TSS=\sum(y_i-\bar{y})^2$ - общая сумма квадратов (Total Sum of Squares)
* $ESS=\sum(\hat{y_i}-\bar{y})^2$ - объясненная сумма квадратов (Explained Sum of Squares)

#### Абсолютный ликбез по линейной алгебре
Маленькой $y$ мы будем обозначать вектор значений:
$$
y=
\begin{pmatrix}
  y_1 \\ y_2 \\ \vdots \\ y_n
\end{pmatrix}
$$

Аналогично для $x$, $\hat\varepsilon$ и дополнительно введеннй вектор $\vec{1}$:

$$
x=
\begin{pmatrix}
  x_1 \\ x_2 \\ \vdots \\ x_n
\end{pmatrix}
\quad
\hat\varepsilon=
\begin{pmatrix}
  \hat\varepsilon_1 \\ \hat\varepsilon_2 \\ \vdots \\ \hat\varepsilon_n
\end{pmatrix}
\quad
\vec{1}=
\begin{pmatrix}
  1 \\ 1 \\ \vdots \\ 1
\end{pmatrix}
$$

Соответственно, если нашу модель записать не для одного наблюдения, а сразу для всех:
$$
\hat y = \hat\beta_1 \cdot \vec{1} + \hat\beta_2 \cdot x + \hat\beta_3 \cdot z
$$

И еще одно обозначение; это **ВСЕ** регрессоры:
$$
X=
\begin{pmatrix}
  1 &x_1 &z_1 \\
  1 &x_2 &z_2 \\
  \vdots \\
  1 &x_n &z_n 
\end{pmatrix}
$$

#### Длина вектора
У векторов есть такое понятие как длина:
$$
|y|=\sqrt{y_1^2+y_1^2+\ldots+y_n^2}
$$

И квадрат длины:
$$
|y|^2=y_1^2+y_1^2+\ldots+y_n^2=\sum_{i=1}^n y_i^2
$$
