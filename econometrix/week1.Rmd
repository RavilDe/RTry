---
title: "Эконометрика"
author: "Демешев Борис Борисович"
output:
  html_document: default
  pdf_document: default
header-includes: \usepackage[english, russian]{babel}
---
```{r setup, include = F}
library(knitr)        # нужен для ф-ии kable
library(grid)
library(ggplot2)
library(dplyr)    # для обработки данных
library(GGally)   # для матриц диаграмм рассеивания
library(psych)    # для описательных статистик
```

### 1.1.1. Суть метода наименьших квадратов
#### Общие сведения
Эконометрика отвечает на два вопроса:

* Как переменная $x$ влияет на переменную $y$
* И как спрогнозировать переменную $y$

Ответы на эти два вопроса мы получаем с помощью моделей.

**Модель** - некая формула, которая связывает объясняемую переменную $y$ и объясняющую переменную $x$ (одну или несколько).

Например модель может иметь вид: $y_i=\beta_1 +\beta_2x_i+\varepsilon_i$

#### Типы данных
Прежде чем говорить об оценивании моделей, выборе модели, надо начать с данных. Без данных моделей не бывает. В эконометрике встречается много разных типов данных, но пожалуй самые простые базовые типы данных, это:

* **Временные ряды** - есть некий индекс времени, например год и показатели, меняющиеся во времени; т.е. это несколько показателей на разные моменты времени; время, как правило регулярны (год, месяц и т.д.).
* **Перекрестные данные** - это когда есть несколько объетов на один момент времени и для каждого объекта есть свой набор показателей (читай сводная таблица).
* **Панельные данные** - это сочетание первых двух, например несколько перекрестных таблиц упорядоченных во временном ряду.

#### Данные - обозначения
* одна зависимая (**объясняемая**) переменная будет обозначаться $y$; это так переменная которую мы хотим предсказать, или понять от чего она зависит.
* несколько регрессоров (**объясняющих** переменных) будут обозначаться $x,z,\dots$
* по каждой переменной будет $n$ наблюдений: $y_1, y_2, y_3, \dots, y_n$

#### Данные - пример
Исторические данные 1920-х годов по автомобилям. Были измерены скорости автомобилей $y_i$ и длины тормозного пути $x_i$. 

При работе с данными **ВСЕГДА** отображайте их. Никакой эконометрический анализ не заменит простой график. Нарисовав график мы увидим ожидаемую взаимосвязь - чем больше скорость, тем больше длина тормозного пути, в среднем. 

#### Модель
В таком случае предположим, что модель имеет простую линейную форму:
$$y_i=\beta_1 +\beta_2x_i+\varepsilon_i$$
У нас есть:

* наблюдаемые переменные: $y, x$
* неизвестные параметры: $\beta_1, \beta_2$
* случайная составляющая, ошибка: $\varepsilon$

План действия:

* придумать адекватную модель - по построенному графику нам вполне подходит линейная модель
* получить оценкин еизвестных параметров: $\hat{\beta_1}, \hat{\beta_2}$; 
другими словами нам нужен некий метод, для получения оценок.
* затем, если мы хотим прогнозировать или интерпретировать наши данные, то можно ипользовать полученные
оценки: $\hat{y_i}=\hat{\beta_1} + \hat{\beta_2}x_i$

**М**етод **Н**аименьших **К**вадратов - это самый простой и самый популярный метод оценки $\hat{\beta_1}, \hat{\beta_2}$.

Суть метода - получить оценки неизвестных параметров, исходя из реальных данных.

Если мы придумали какие-то оценки $\hat{\beta_1}, \hat{\beta_2}$, то естественно возникает такое понятие как **Ошибка прогноза**:
$$\hat{\varepsilon_i}=y_i - \hat{y_i}$$
Отсюда возникает суммарная ошибка прогноза:
$$Q(\hat{\beta_1}, \hat{\beta_2})=\sum_{i=1}^n{\hat{\varepsilon_i}^2}=\sum_{i=1}^n{(y_i - \hat{y_i})^2}$$
Чтобы ошибки не компенсировали друг друга (одна в плюс, другая в минус) мы возведем их в квадрат и посчитаем сумму квадратов ошибок прогноза.

#### Суть МНК
Возьмите в качестве оценок такие $\hat{\beta_1}, \hat{\beta_2}$, при которых сумма квадратов ошибок прогноза $Q$ минимальна.

#### Пример с машинами
Прогнав данные через **R**, мы получили:
$$\hat{\beta_1}=-5.3,\quad \hat{\beta_2}=0.7$$
$$\hat{y_i}=-5.3 +0.7x_i$$

### 1.1.2. Пример. Регрессия на константу (у доски)
Простой набор данных: есть три человека, их рост и вес.

<!-- $$ -->
<!-- \begin{table} -->
<!-- {rcc} -->
<!--  & $y_i$ (вес, кг) & $x_i$ (рост, см) \\ -->
<!--  Вася & 60 & 170 \\ -->
<!--  Коля & 70 & 170 \\ -->
<!--  Петя & 80 & 181 -->
<!-- \end{table} -->
<!-- $$ -->

```{r, echo = F}
kable(data.frame(c("Вася", "Коля", "Петя"), 
                 c(60,70,80),
                 c(170, 170, 181)),
      col.names = c("", "рост, см", "вес, кг"))
```

Модель №1: $y_i=\beta +\varepsilon_i$

Модель №2: $y_i=\beta_1 +\beta_2 x_i + \varepsilon_i$

Нам необходимо получить $\beta;\beta_1, \beta_2$ с помощью **МНК**.

Первая модель предполгаает, что вес не зависит от роста, т.е. вес человека это некая константа плюс случайная составляющая для каждого человека.

Вторая модель предполагает, что вес зависить от роста линейно.

Нам нужно оценить эти две модели.

Что делает МНК? Он минимизирует RSS: $min\sum{(y_i-\hat{y_i})^2}$, подбирая коэффициенты $\hat\beta$.

#### Модель №1
Вместо настоящего коэф. пишем его оценку, а ошибка $\varepsilon$ вообще не прогнозируема, пишем вместо неё ноль.
$$\hat{y_i}=\hat{\beta}$$

Тогда получаем:
$$
\begin{gathered}
  \sum_{i=1}^3{(y_i-\hat{y_i})^2}=\sum_{i=1}^3{(y_i-\hat{\beta})^2}=\\
  =(y_1-\hat{\beta})^2-(y_2-\hat{\beta})^2-(y_3-\hat{\beta})^2=Q(\hat{\beta})
\end{gathered}
$$
Давайте раскроем скобки под суммой:
$$
\begin{gathered}
  \sum_{i=1}^3{(y_i^2-2\hat{\beta}y_i + \hat{\beta^2})} \\
  =\sum_{i=1}^3{y_i^2}-\sum_{i=1}^3{2\hat{\beta}y_i}+\sum_{i=1}^3{\hat{\beta^2}}
\end{gathered}
$$
Заметим, что последняя сумма не зависит от $i$;
$$
\sum_{i=1}^3{\hat{\beta^2}}=\hat{\beta^2}+\hat{\beta^2}+\hat{\beta^2}=3\hat{\beta^2}
\qquad (n)\hat{\beta^2} 
$$

Во втором слагаемом выносим $2\hat{\beta}$ за знак суммы:
$$
\sum_{i=1}^3{2\hat{\beta}y_i}=2\hat{\beta} \sum_{i=1}^3{y_i}
$$
Выведем универсальную формулу для первой модели (для кол-ва переменных $n$):
$$
Q(\hat{y_i})=\sum{y_i^2}-2\hat{\beta}\sum{y_i}+n\hat{\beta^2}
$$

И возьмем от нее производную для поиска экстремума:
$$
Q'(\hat{\beta})=-2\sum{y_i}+2n\hat{\beta}
$$

Заметим, что наша функция $Q(\hat{\beta})$ является параболой с ветвями направленными вверх, т.к. коэф $n$ при $\hat{\beta}$ является положительным (это кол-во наблюдений).

Приравняем производную $Q(\hat{\beta})$ к нулю:

$$
Q'(\hat{\beta})=-2\sum{y_i}+2n\hat{\beta}=0
$$

Выражаем отсюда $\hat{\beta}$:
$$
\hat{\beta}=\frac{\sum{y_i}}{n}=\bar{y}
$$
Получем среднее значение:
$$
M1: \quad \hat{\beta}=\frac{60+70+80}{3}=70
$$

### 1.1.3. - 1.1.4. Пример 2. Парная регрессия.
Перейдем к оценке второй более сложной модели Методом Наименьших квадратов.

Для начала нам понадобится некое вспомогательное наблюдение, чтобы легче было следовать за выкладками. 

Заметим, что если посчитать среднее:
$$
\frac{\sum_{i=1}^n{x_i}}{n}=\bar{x}
$$
Из этого следует:
$$
\sum_{i=1}^n{x_i}=n\cdot\bar{x}=\sum_{i=1}^n{\bar{x}}
$$
Эти две суммы можно записать еще вот так:
$$
\sum_{i=1}^n(x_i-\bar{x})=0
$$
Вооружившись таким знанием мы готовы переходить к оценке второй модели:
$$
RSS=\sum_{i=1}^n(y_i-\bar{y})^2
$$
Для второй модели:
$$
M2: \quad \hat{y_i}=\hat\beta_1 + \hat\beta_2 x_i
$$
Подставляем $\hat y$ в формулу с квадратом ошибок:
$$
RSS=\sum_{i=1}^n(y_i-\hat\beta_1 -\hat\beta_2x_i)^2=Q(\hat\beta_1,\hat\beta_2)
$$
Минимизируем эту функцию $Q(\hat\beta_1,\hat\beta_2)$ взяв производную:
$$
\left\{
  \begin{aligned}
    \frac{\partial Q}{\partial \hat\beta_1} &=
    \sum 2 \cdot (y_i-\hat\beta_1 -\hat\beta_2x_i) \cdot (-1) = 0 \\
    \frac{\partial Q}{\partial \hat\beta_2} &=
    \sum 2 \cdot 
      \underbrace{(y_i-\hat\beta_1 -\hat\beta_2x_i)}_{\Large\hat\varepsilon_i} 
    \cdot (-x_i) = 0
  \end{aligned}
\right.
$$
Это условия первого порядка и из этой мимтемы уравнений у нас $y_i$ даны, $x_i$ даны, в этих двух уравнениях неизвестны только $\hat\beta_1, \hat\beta_2$.
$$
\left\{
  \begin{aligned}
    &\sum_{i=1}^n \hat\varepsilon_i \cdot 1=0 \\
    &\sum_{i=1}^n \hat\varepsilon_i \cdot x_i=0
  \end{aligned}
\right.
$$
Решаем первую систему уравнений; сократим на $-2$ и раскроем скобки:
$$
\left\{
  \begin{aligned}
    & \sum y_i - \sum \hat\beta_1 - \sum \hat\beta_2 x_i=0 \\
    & \sum y_i \cdot x_i - \sum \hat\beta_1 \cdot x_i - \sum \hat\beta_2 \cdot x_i^2 =0
  \end{aligned}
\right.
$$
Вынесем константы за знак суммы и поделим на $n$:
$$
\left\{
  \begin{aligned}
    &\sum y_i - n\cdot\hat\beta_1 - \hat\beta_2\cdot \sum x_i = 0 \\
    &\sum y_i \cdot x_i - \hat\beta_1 \cdot x_i - \hat\beta_2\cdot \sum x_i^2 = 0
  \end{aligned}
\right.
$$
Делим первое уравнение в системе на $n$ и получаем:
$$
\begin{gathered}
  \bar y - \hat\beta_1 - \hat\beta_2 \cdot \bar x=0 \\
  \text {или} \\
  \bar y = \hat\beta_1 + \hat\beta_2 \cdot \bar x
\end{gathered}
$$

Выражаем $\hat\beta_1=\bar{y} -\hat\beta_2 \cdot \hat{x}$ и подставляем во второе уравнение:
$$
\begin{gathered}
  \sum{y_i x_i}-(\bar{y}- \hat\beta_2 \bar{x})\cdot\sum{x_i}-\hat\beta_2\cdot{x_i}^2=0 \\
  \sum{y_i x_i}-\bar{y}\cdot\sum{x_i}+\hat\beta_2\cdot\bar{x}\cdot\sum{x_i}-
    \hat\beta_2\cdot{x_i}^2=0 \\
  \hat\beta_2(\bar{x}\sum{x_i}-\sum{x_i^2})=\bar{y}\cdot\sum{x_i}-\sum{y_i x_i} \\
  \hat\beta_2=\frac{\bar{y}\cdot\sum{x_i}-\sum{y_i x_i}}{\bar{x}\sum{x_i}-\sum{x_i^2}}
\end{gathered}
$$

Приведем конечную формулу к более классическому виду; помножим числитель и знаменатель на $-1$; так же внесем под знак суммы $\bar{y}, \bar{x}$, т.к. это константы:
$$
\begin{gathered}
  \hat\beta_2=\frac{\bar{y}\cdot\sum{x_i}-\sum{y_i x_i}}{\bar{x}\sum{x_i}-\sum{x_i^2}} =
  \qquad \times (-1) \\
  =\frac{\sum{y_ix_i}-\sum{\bar{y}x_i}}{\sum{x_i^2}-\sum{\bar{x}x_i}}=
  \frac{\sum(y_i-\bar{y})x_i}{\sum(x_i-\bar{x})x_i}
\end{gathered}
$$
Вспомним, что $\sum(x_i-\bar{x})=0$; соответственно, если домножить сумму на любой показатель (допустим на $\bar{x}$), то она тоже будет равняться нулю: 
$$
\bar{x}\sum(x_i-\bar{x})=0 \Rightarrow \sum\bar{x}(x_i-\bar{x})=0
$$

Вычтем подобные "нули" из числителя и знаменателя и далее внесем все под один знак суммы:
$$
\begin{gathered}
  \hat\beta_2=
    \frac
    {\sum(y_i-\bar{y})x_i-\overbrace{\sum\bar{x}(y_i-\bar{y})}^{0}}
    {\sum(x_i-\bar{x})x_i-\underbrace{\sum\bar{x}(x_i-\bar{x})}_{0}}= \\
  =\frac{\sum(y_i-\bar{y})\cdot(x_i-\bar{x})}{\sum(x_i-\bar{x})^2}
\end{gathered}
$$
Эта форма записи xороша тем, что везде фигурирует отклонение наблюдений от среднего значения.


Если подставить все $x_i$ и $y_i$, то получим следующте оценки:
$$
\hat\beta_2 = 1,36 \qquad \hat\beta_1 = -166,8
$$
Таким образом мы получили две формулы для $\hat\beta_1, \hat\beta_2$

### 1.1.5 МНК на графике. Случай множеста регрессоров.
Применив МНК к двум простым моделям мы получили следующие результаты:

В модели $y_i=\beta+\varepsilon_i$:
$$\hat\beta=\bar{y}$$
В линейной модели $y_i=\beta_1+\beta_2x_i+\varepsilon_i$:
$$
\hat\beta_1=\bar{y} -\hat\beta_2 \cdot \hat{x}
\qquad
\hat\beta_2=\frac{\sum(y_i-\bar{y})\cdot(x_i-\bar{x})}{\sum(x_i-\bar{x})^2}
$$

Первое уравнение означает, что линия регресии $\hat\beta_1=\bar{y} -\hat\beta_2 \cdot \hat{x}$ проходит через точку $(\bar{x}, \bar{y})$ 

#### Подведем итог используемой терминологи.
$y_i$ - зависимая, объясняемая переменная;

$x_i$ - регрессор, объясняющая переменная;

$\varepsilon_i$ - ошибка, ошибка модели, случайная составляющая, непредсказумая/немоделируемая величина;

$\hat y_i$ - прогноз, прогнозное значение;

$\hat{\varepsilon_i}=y_i-\hat{y_i}$ - остаток, ошибка прогноза;

$RSS = \sum_{i=1}^n{\hat\varepsilon_i^2}$ - сумма квадратов остатков или сумма квадратов ошибок прогноза (Residual Sum of Squares);

#### <span style="color:red">Далее следует отрывок с графическим пояснением МНК - НАРИСОВАТЬ!!!!!!!!!!!!!!!!</span>

#### Много объясняющих переменных.
Вринципе, дважцать случай с двадцатью объясняющими переменными концептуально ничем не отличается от случая с двумя объясняющими переменными.

$$
y_i=\beta_1+\beta_2 x_i + \beta_2 z_i +\varepsilon_i
$$
Наша задача выписать систему уравнений для оценок $\hat\beta_1,\hat\beta_2,\hat\beta_3$.

У нас имеется модель:
$$
y_i=\beta_1+\beta_2 x_i + \beta_2 z_i +\varepsilon_i
$$
И прогноз для нее:
$$
\hat{y_i}=\hat\beta_1+\hat\beta_2 \hat{x_i} + \hat\beta_2 \hat{z_i}
$$
ошибка прогноза:
$$
\hat\varepsilon_i=y_i-\hat{y_i}
$$
И Метод Наименьших Квадратов минимизирует ошибку прогноза:
$$
\text{МНК: } \min \sum \hat\varepsilon_i^2=Q(\hat\beta_1,\hat\beta_2,\hat\beta_3)
$$
Заметим, что $y_i$ не зависит от $\hat\beta_1,\hat\beta_2,\hat\beta_3$, от них зависят только прогнозы. Тогда возьмем производные от $\hat\varepsilon_i$ по $\hat\beta_1,\hat\beta_2,\hat\beta_3$:
$$
\left\{
  \begin{aligned}
    \frac{\partial\hat\varepsilon}{\partial\hat\beta_1} =-1
    \quad
    \frac{\partial\hat\varepsilon}{\partial\hat\beta_2} =-x_i
    \quad
    \frac{\partial\hat\varepsilon}{\partial\hat\beta_3} =-z_i
  \end{aligned}
\right.
$$
Теперь можно выписать условия первого порядка:
$$
\left\{
  \begin{aligned}
    \frac{\partial Q}{\partial\hat\beta_1}=0 
    \quad
    \frac{\partial Q}{\partial\hat\beta_2}=0
    \quad
    \frac{\partial Q}{\partial\hat\beta_3}=0
  \end{aligned}
\right.
$$
Возьмем производную от суммы квадратов ошибок:
$$
\left\{
  \begin{aligned}
    & \sum 2 \cdot \hat\varepsilon_i \cdot (-1)=0  \\
    & \sum 2 \cdot \hat\varepsilon_i \cdot (-x_i)=0  \\
    & \sum 2 \cdot \hat\varepsilon_i \cdot (-z_i)=0  
  \end{aligned}
\right.
$$
$$
\left\{
  \begin{aligned}
    & \sum \hat\varepsilon_i \cdot 1=0  \\
    & \sum \hat\varepsilon_i \cdot x_i=0  \\
    & \sum \hat\varepsilon_i \cdot z_i=0  
  \end{aligned}
\right.
$$

Мы получили условия первого опрядка и из этой системы можно найти три неизвестных
$\hat\beta_1,\hat\beta_2,\hat\beta_3$.

### 1.1.6 Ликбез по линейной алгебре 
Среди прочих показателй, рассчитываемых в МНК, фигурирует три суммы квадратов. Эти суммы квадратов меряют изменчивость нескольких показателей:

* $RSS=\sum\hat\varepsilon_i^2$ - сумма квадратов остатков (Residual Sum of Squares)
* $TSS=\sum(y_i-\bar{y})^2$ - общая сумма квадратов (Total Sum of Squares)
* $ESS=\sum(\hat{y_i}-\bar{y})^2$ - объясненная сумма квадратов (Explained Sum of Squares)

#### Абсолютный ликбез по линейной алгебре
Маленькой $y$ мы будем обозначать вектор значений:
$$
y=
\begin{pmatrix}
  y_1 \\ y_2 \\ \vdots \\ y_n
\end{pmatrix}
$$

Аналогично для $x$, $\hat\varepsilon$ и дополнительно введеннй вектор $\vec{1}$:

$$
x=
\begin{pmatrix}
  x_1 \\ x_2 \\ \vdots \\ x_n
\end{pmatrix}
\quad
\hat\varepsilon=
\begin{pmatrix}
  \hat\varepsilon_1 \\ \hat\varepsilon_2 \\ \vdots \\ \hat\varepsilon_n
\end{pmatrix}
\quad
\vec{1}=
\begin{pmatrix}
  1 \\ 1 \\ \vdots \\ 1
\end{pmatrix}
$$

Соответственно, если нашу модель записать не для одного наблюдения, а сразу для всех:
$$
\hat y = \hat\beta_1 \cdot \vec{1} + \hat\beta_2 \cdot x + \hat\beta_3 \cdot z
$$

И еще одно обозначение; это **ВСЕ** регрессоры:
$$
X=
\begin{pmatrix}
  1 &x_1 &z_1 \\
  1 &x_2 &z_2 \\
  \vdots \\
  1 &x_n &z_n 
\end{pmatrix}
$$

#### Длина вектора
У векторов есть такое понятие как длина:
$$
|y|=\sqrt{y_1^2+y_1^2+\ldots+y_n^2}
$$

И квадрат длины:
$$
|y|^2=y_1^2+y_1^2+\ldots+y_n^2=\sum_{i=1}^n y_i^2
$$
Примеры, где у нас возникают кавдраты длин:

$RSS=\sum\hat\varepsilon_i^2$ - это квадрат длины вектора $\hat\epsilon$;

$TSS=\sum(y_i-\bar{y})^2$ - квадрат длины вектора $(y-\hat{y}\cdot\vec{1})$:

$$
\begin{pmatrix}
  y_1 - \bar{y} \\ y_2 - \bar{y} \\ \vdots \\ y_n-\bar{y}
\end{pmatrix}
=
\begin{pmatrix}
  y_1 \\ y_2 \\ \vdots \\ y_n
\end{pmatrix}
-\bar{y}
\begin{pmatrix}
  1 \\ 1 \\ \vdots \\ 1
\end{pmatrix}
= y-\bar{y} \cdot \vec{1}
$$

#### Скалярное произвдедение двух векторов
Скалярное произведение двух векторов - это произведение длин этих векторов и косинуса угла между ними:
$$
(x,y)=|x| \cdot |y| \cdot cos(x,y)
$$
Либо сумма попарных произведений координат вектора: 
$$
(x,y)=x_1y_1 +x_2y_2 + \dots + x_ny_n = \sum_i{x_iy_i}
$$

Само скалярное произведение тяжело интерпретировать; но оно помогает понят перпендикулярны ли векторы!
$$
\begin{gathered}
  x \bot y, \quad \sum_i{x_iy_i}=0,\\
  \text{т.к.} \quad cos(90^{\circ})=0
\end{gathered}
$$

#### 1.1.7 <span style="color:red">Далее лектор рисует картинку в n-мерном пространстве.</span>

#### 1.1.8 Геометрическая иллюстрация МНК для множества регрессоров
Модель:
$$
y_i=\beta_1+\beta_2 x_i + \beta_2 z_i +\varepsilon_i
$$

Геометрическая интерпретация условий первого порядка:
$$
\left\{
  \begin{aligned}
    &\sum\hat\varepsilon_i\cdot 1=0 \\
    &\sum\hat\varepsilon_i\cdot x_i=0 \\
    &\sum\hat\varepsilon_i\cdot z_i=0
  \end{aligned}
\right.
\quad \Leftrightarrow
\left\{
  \begin{aligned}
    &\hat\varepsilon \bot \vec{1} \\
    &\hat\varepsilon \bot x \\
    &\hat\varepsilon \bot z
  \end{aligned}
\right.
$$
Соответственно мы можем проиллюстрировать метод наименьших квадратов для случая множества объясняющих переменных.

Заодно на этом рисунке мы увидим, где находится RSS, TSS и ESS.

Проиллюстрируем оценку модели: $y_i=\beta_1+\beta_2x_i+\beta_3z_i+\varepsilon_i$ с помощью метода наименьших квадратов.

Условие первого порядка для оценки неизвестных коэф $\beta_1, \beta_2, \beta_3$:
$$
\left\{
  \begin{aligned}
    &\hat\varepsilon \bot \vec{1} \\
    &\hat\varepsilon \bot x \\
    &\hat\varepsilon \bot z
  \end{aligned}
\right.
\Rightarrow \hat\beta_1, \hat\beta_2, \hat\beta_3
$$
Так же вспоминаем, что $\hat\varepsilon_i=y_i-\hat{y_i}$; отсюда следует, что 
$y_i=\hat{y_i}+\hat\varepsilon_i$. Переведем последнее выражение в векторную форму:
$\boxed{y=\hat y +\hat\varepsilon}$. Этот факт тоже хорошо иллюстрируется геометрически:

```{r, echo = F, fig.align = 'center', fig.width = 3, fig.height = 2}
sml <- data.frame(name = rep(c("y", "y_hat", "eps"), each = 2),
                  x = c(0, 1, 0, 1, 1, 1),
                  y = c(0, 1, 0, 0, 0, 1))
annotext <- data.frame(x = c(0.5, 0.55, 0.95),
                       y = c(0.62, 0.12, 0.47),
                       label = c("y", "hat(y)", "hat(epsilon)"),
                       color = c("y", "y_hat", "eps"))
myarrow <- arrow(angle = 15, ends = "last", type = "closed") # grid library
ggplot(data = sml, aes(x, y)) +
  geom_line(aes(group = name, color = name), arrow = myarrow) +
  # bспользуем вторую aes из data.frame annotext
  geom_text(data = annotext,
            aes(x = x, y = y, label = label, color = color, size = 6),
            parse = T) +
  # # Можно еще вот так (много повторов, второй aes решает эту проблему):
  # geom_text(aes(x = 0.5, y = 0.62, label = "y", color = "y", size = 6)) +
  # geom_text(aes(x = 0.55, y = 0.12, label = "hat(y)", color = "y_hat", size = 6), parse = T) +
  # geom_text(aes(x = 0.95, y = 0.47, label = "hat(epsilon)", color = "eps", size = 6), parse = T) +
  # # Или вот так; это изящней всего, но цвет так и не смог прикрутить(((
  # annotate("text",
  #          size = 6,
  #          parse = TRUE,
  #          # color = "red",
  #          x = c(0.5, 0.55, 0.95),
  #          y = c(0.62, 0.12, 0.47),
  #          label = c("y", "hat(y)", "hat(epsilon)")) +
  theme_classic() +
  theme(line = element_blank(),
        text = element_blank(),
        title = element_blank(),
        legend.position = "none") +
  scale_color_brewer(palette = "Dark2")
```

Напоминаем, что: $\hat{y}=\hat\beta_1\cdot \vec{1} +\hat\beta_2 \cdot x +\hat\beta_3 \cdot z$

Теперь можно проиллюстрировать.

Я напомню, что пространство у нас $n$-мерное, $n$ — это количество наблюдений. Поэтому... И $n$, как правило, велико. Например, 100 наблюдений. Поэтому, например, в этом пространстве мы можем нарисовать 20 векторов, перпендикулярных друг другу, и при этом все попарно будут перпендикулярны друг другу. Это вектор $x$. Это вектор $z$. Это будет вектор $y$. Во-первых, я все множество векторов, которое можно получить, складывая с каким-то весом вектор $x$, вектор $z$, вектор из единичек, обозначу вот таким вот облачком. То есть облачко — это все те вектора, которые можно получить, складывая с некоторыми весами вектор $x$, вектор $z$ и вектор из единичек. Итак, как я геометрически интерпретирую условия первого порядка и факт, что $\hat\varepsilon$ — это разница между $y$ и $\hat{y}$. Во-первых, $\hat{y}$ можно выразить через вектор из единичек, через вектор $x$ и через вектор $z$.


#### 1 геометрический факт:
**МНК**: $\hat{y}$ - это проекция вектора зависимых переменных $y$ на множество векторов,
получаемых с помощью сложения с разными весами векторов $\vec{1}, x, y$. 

#### 2 геометрический факт:
Из условия первого порядка, что $\hat\varepsilon \bot \vec{1}$ следует:
$$
\begin{gathered}
  \sum\hat\varepsilon_i=0 \\
  \sum y_i-\hat y_i=0 \\
  \sum y_i = \sum\hat y_i \\
  \bar y = \bar{\hat{y}}
\end{gathered}
$$

т.е. среднее значение фактических переменных равно среднему значению прогнозов. Но мы знаем, что среднее значение можно получить, спроецировав любой вектор на прямую, порождаемую вектором из единичек. 

<span style="color:red">Далее лектор рисует адовую картинку в n-мерном пространстве!!!</span>

#### 3 геометрический факт:
Треугольник с вершинами $A \rightarrow B \rightarrow C$ или $y \rightarrow \bar{y} \cdot \vec{1} \rightarrow \hat{y}$:
$$
\begin{gathered}
  {AB}^2={AC}^2+{BC}^2 \\
  \\
  {AC}^2={|\hat\varepsilon|}^2=\sum{\varepsilon_i}^2 = RSS \\
  {AB}^2={|y-\bar{y}\cdot \vec{1}|}^2 = \sum(y_i-\bar{y})^2=TSS \\
  {BC}^2={|\hat{y}-\bar{y}\cdot \vec{1}|}^2 = \sum(\hat{y_i}-\bar{y})^2=ESS
\end{gathered}
$$

Отсюда следует, что между показателями RSS, TSS и ESS имеет место следующее соотношение:
$$
{TSS}={RSS}+{ESS}
$$

#### 4 геометрический факт:
$$
\frac{ESS}{TSS}=\frac{{BC}^2}{{AB}^2}=\left(\frac{BC}{AB}\right)^2=(cos\varphi)^2
$$

### 1.1.9 Коэффициент детерминации

Мы проиллюстрировали метод наименьших квадратов для множественной регрессии, когда у нас много регрессоров. И попутно мы обнаружили следующие факты

Если в регрессию включен свободный член $(y_i=\beta_1+\dots)$, и оценки **МНК** единственны, то:

$\sum\hat\varepsilon_i=0$

$\sum y_i=\sum \hat{y_i}$

$y=\hat{\bar y}$

$TSS=RSS+ESS$

Наличие последнего разложения позволяет придумать простой показатель качества модели:

*$R^2$* - коэффицинт детерминации

#### В моделях со свободным членом:

$TSS=RSS+ESS$

$TSS = \sum(y_i-\bar{y})^2 - \text{ общий разброс } y$

$ESS = \sum(\hat{y_i}-\bar{y})^2 - \text{ объясненный регрессорами разброс}$

$RSS = \sum{\hat\varepsilon_i^2} - \text{ доля объясненного разброса в общем разбросе}$

Чем прогнозы точнее и похожи на настоящие $y$ тем меньше будут ошибки прогнозов $\hat\varepsilon_i$ и меньше $RSS$

Соответственно отношение $ESS$ к $TSS$ или $R^2$ будет стремиться к $1$ (при $RSS$ примерно равной нулю).

Коэффициент детерминации $R^2 \in [0;1]$

$R^2=ESS/TSS$ - доля объясненного разброса $y$ в общем разбросе $y$



#### Интерпретация $R^2$

Теорема

Если в регрессию включен свободный член $(y_i=\beta_1 + \dots)$ и оценки МНК единственны , то $R^2$ равен квадрату выборочной корреляции между $y$ и $\hat y$, т.е. 

$$
\begin{gathered}
  R^2=(sCorr(y,\hat y))^2= \\
  =\left(
    \frac
    {\sum(y_i-\bar{y})(\hat{y_i}-\bar{y})}
    {\sqrt{\sum(y_i-\bar{y})^2}\sqrt{\sum(\hat{y_i}-\bar{y})^2}}
  \right)^2
\end{gathered}
$$
Коэффициент детераминации - это $cos^2(\varphi)$

$$
\begin{gathered}
  R^2=\frac{ESS}{TSS} \in[0;1] \\
  R^2=cos^2(\varphi)
\end{gathered}
$$
Для косинуса у нас есть другая формула. Т.к. для скалярное произведение любых дыух векторов:
$$
\begin{gathered}
  (a,b)=|a|\cdot |b| \cdot cos(a,b)\\
  \Downarrow \\
  cos(a,b)=\frac{(a,b)}{|a|\cdot |b|}
\end{gathered}
$$
Угол $\varphi$ это угол между $BA$ и $BC$. Тогда вектора можно задать:
$$
\begin{gathered}
  \vec{BA}=y-\bar{y}\cdot \vec{1} \\
  \vec{BC}=\hat{y}-\bar{y}\cdot \vec{1}
\end{gathered}
$$

Нам нужен $cos$ углоа между этими векторами.

$$
\begin{gathered}
  (\vec{BC},\vec{BA})=(\hat{y}-\bar{y}\cdot \vec{1},y-\bar{y}\cdot \vec{1})= \\
  =\sum(\hat{y_i}-\bar{y})(y_i-\bar{y})
\end{gathered}
$$

А теперь знаменатель:
$$
\begin{gathered}
  |\vec{BC}|=\sqrt{\sum(\hat{y_i}-\bar{y})^2} \qquad
  |\vec{BA}|=\sqrt{\sum(y_i-\bar{y})^2}
\end{gathered}
$$
Таким образом мы получили еще одну формулу для $R^2$:
$$
R^2=
  \left(
    \frac
    {\sum(\hat{y_i}-\bar{y})(y_i-\bar{y})}
    {\sqrt{\sum(\hat{y_i}-\bar{y})^2 \cdot\sum(y_i-\bar{y})^2}}
  \right)^2=
(sCorr(y,\hat{y}))^2
$$
$sCorr$ - выборочная корреляция (sample correlation) двух векторов.


### 1.1.10 Мораль первой лекции
Мы с вами выяснили, что метод наименьших квадратов позволяет нам получить оценки 
коэффициентов в линейной модели. При этом оценки метода наименьших квадратов получаются как решение некоторой системы линейных уравнений. Существует целая наука, которая отчасти занимается решением систем линейных уравнений, — это линейная алгебра.

Модель: $y_i=\beta_1+\beta_2x_i + \beta_3z_i + \varepsilon_i$
$$
y=
\begin{pmatrix}
  y_1 \\ y_2 \\ \vdots \\ y_n
\end{pmatrix}
\qquad
X=
\begin{pmatrix}
  1 &x_1 &z_1 \\
  1 &x_2 &z_2 \\
  \vdots \\
  1 &x_n &z_n 
\end{pmatrix}
$$
$\hat\beta=(X'X)^{-1}X'y$

И пользуясь методами линейной алгебры, можно показать, что оценки β с крышкой в терминологии линейной алгебры, имеют следующий вид: это (X'X) в минус первой умножить на X'y. Соответственно, имея готовую модель, мы можем ее оценить методом наименьших квадратов. Мы пока совсем не касались множества вопросов. Во-первых, откуда взять эту самую модель? Во-вторых, даже если мы возьмем модель, а будет ли решение задачи минимизации, будут ли оценки единственными, может быть будет несколько вариантов различных β с крышкой? А будет ли решение задачи вообще существовать? А почему мы взяли сумму квадратов остатков, а не, скажем, модулей? А насколько полученные оценки β с крышкой, насколько они действительно близки к настоящим неизвестным β? В следующих занятиях мы постараемся ответить на все эти вопросы.

### Консольный режим в R
Консольный режим - режим команда-вывод

```{r}
5 + 6 # используем консольный режим аки калькулятор
factorial(10) # чуть посложнее))

# присвоим переменную
b <- 11 # стиль диктует!
5 -> a  # потому что можно и так
a + b   # и все работает

# R чувствителен к регистру
A <- 55
a + A  # выдаст 60
```
R Studio умеет дополнять Tab'ом; при написании переменных, функций и их аргументов.

#### Основная вишка R - работа с векторами:
```{r}
y <- c(3, -2, 5, 6, NA, 9)
y + 2
```
#### Работа с бесконечностями:
```{r}
0/0 # выдаст NaN - not a number
1/0 # выдаст Inf - бесконечность
1/Inf # выдаст 0
atan(Inf) # pi/2
pi/2      # действительно сходится
```

#### Создание вектора последовательности:
```{r}
z <- 200:300
z
```

### 1.2.2 Написание первого скрипта в R

В консольном режиме не удобно выполнять длинные последовательности команд, поэтому имеет смысл набирать команды в одном файле - скрипте.
```{r}
# Создадим два вектора
x <- c(23, 15, 46, NA)
z <- c(5, 6, NA, 8)

mean(x) # из=за  наличия NA выдаст NA
mean(x, na.rm = T) # пропускаем NA
mean(z, na.rm = T) # пропускаем NA

sum(x)
sum(x, na.rm = T)

```
#### Следующий объект который часто используется в R это табличка с данными.

Объединим два вектора в таблицу:
```{r}
d <- data.frame(rost = x, ves = z)
d
```

#### Адресовка двумерного объекта:
```{r}
d[4, 1]
d[3, 1]

d[2,] # вся строка
d[,2] # весь столюец

d$rost
d$ves
```
#### Списки

Этакий чулан с хламом (: 
```{r}
mylist <- list(a = 7, b = 10:20, table = d)
mylist
```
### 1.2.3 Установка пакетов R. Получение справки.

Список пакетов, необходимых нам далее:
```{r, eval = F}
library(dplyr)    # для обработки данных
library(ggplot2)  # для магии с графиками
library(GGally)   # для матриц диаграмм рассеивания
library(psych)    # для описательных статистик

library(devtools) # нужен для установки пакетов с гитхаба
install.packages("bdemeshev/sophisthse") # имя пользователя / название репозитория

```
##### Справка
```{r}
?predict
?describe # из пакета psych
help(predict)
?lm
```

### 1.2.4 Первый взгляд на набор данных в R.
```{r}
d <- cars
str(d) # структура данных
glimpse(d) # выдает бооольше данных

?cars # просто читаем про набор данных
head(d) # первые 6 строк
tail(d) # последние 6 строк

describe(d) # описательные статистики
ncol(d) # число переменных
nrow(d) # число наблюдений

mean(d$speed)

# Приведем данные в божеский вид
d2 <- mutate(d, 
             speed = 1.67 * speed, 
             dist = 0.3 * dist,
             ratio = dist/speed)
glimpse(d2)
```

Перед тем как строить регрессии или оценкам моделей необходимо просто посмотреть на графики:
```{r, message = F}
qplot(data = d2, dist)
```

Добавим подписи над осями и название графика:
```{r, message = F}
qplot(data = d2, 
      dist,
      xlab = "длина тормозного пути",
      ylab = "Количество машин",
      main = "Данные 1920х годов")
```

Теперь давайте построим простой дмумерный график:
```{r}
qplot(data = d2, speed, dist)
```

И получаем естественную зависимость: чем болше скорость, тем выше тормозной путь.

### 1.2.5 МНК в R. Пример с машинами.

После того, как мы графически посмотрели на наши данные, мы можем перейти к оценке моделей. Ну, в данном случае, поскольку мы понимаем направление причинно-следственной связи, что это скорость влияет на длину тормозного пути, то, соответственно, мы построим зависимость длины тормозного пути от скорости, то есть оценим модель линейной регрессии.

Это выполняется в R одной командой $lm$ (linear model):
```{r}
# заключая команду в круглые скобки мы сразу выводим ее результат
(model <-  lm(data = d2, dist ~ speed)) 
# зависимая переменная пишется сначала, а после тильды через плюс пишутся все регрессоры
```
И мы видим оценки коэффициентов: $\hat\beta_1 = -5.2737, \hat\beta_2=0.7064$. Это означает примерно следующее: при увеличении скорости на 1 км/ч длина тормозного пути увеличивается на 0,7 м. Intersept - пересечение с нулем, т.е. это длин тормозного пути при скоростит равной нулю. Так вот вышло (:

Давайте извлечем из модели оба коэффициента:
```{r}
(beta_hat <- coef(model))
```

Можно извлечь остатки:
```{r}
(eps_hat <- residuals(model))
```

Давайте выделим сам $y$ из наших исходных данных:
```{r}
(y <- d2$dist)
```

А $\hat y$ можно извлечь из модели:
```{r}
(y_hat <- fitted(model)) # fitted - "подогнанный"
```

Извлекаем дальше (:
```{r}
(RSS <- deviance(model))
```

Для TSS отдельной команды нет, но его просто посчитать. TSS это сумма квадратов отклонения y от среднего.
```{r}
(TSS <- sum((y - mean(y))^2))
```

Ну и посчитаем ESS:
```{r}
(ESS <- TSS - RSS)
```

Два способа расчета $R^2$
```{r}
(R2 <- ESS/TSS )
(R2 <- cor(y, y_hat)^2) # выборочная корреляция
```

Еще можно извлечь матрицу $X$:
```{r}
(X <- model.matrix(model))
```

Ну а теперь, после того как мы все извлекли из модели, можно и попрогнозировать:
```{r}
# создадим новый набор данных, состоящий из двух наблюдений 40 и 60 км/ч
(nd <- data.frame(speed = c(40, 60)))
```

ПРОГНОЗИРУЕМ!
```{r}
(predict(model, nd))
```

Нарисуем наши результаты оценивания моджели
```{r}
qplot(data = d2, speed, dist) + 
  stat_smooth(method = "lm")
```

### 1.2.6. МНК в R. Пример с фертильностью.
Возьмем другой набор данных. И посмотрим как зависит фертильность женщин от других показателей по кантону.
```{r}
t <- swiss
?swiss
glimpse(t) # быстрый взгляд рна данные
describe(t) # описательные статистики

ggpairs(t) # позволяет нарисовать диаграммы орассеивания сразу по всем переменным в наборе данных

```

Оценим модель
```{r}
model2 <- lm(data = t,
            Fertility ~ Agriculture + Education + Catholic)
coef(model2)
```
То есть видно, что в нашей модели при прочих равных доля мужчин, занятых в сельском хозяйстве, отрицательно сказывается на фертильности, образование — отрицательно, а доля католического населения положительно сказывается на показателе фертильности.

Посмотрим остальные результаты:
```{r}
fitted(model2) # y_hat
residuals(model2) # eps_hat
deviance(model2) # RSS

# третий способ получить R2
report <- summary(model2) # отчет по модели
report$r.squared  # R2

cor(t$Fertility, fitted(model2))^2 # корреляция между исходной и спрогнозированной переменной (R2)

```

Давайте снова испробуем модель на новых данных:
```{r}
nd2 <- data.frame(Agriculture = 0.5, 
                  Catholic = 0.5, 
                  Education = 20)
predict(model2, nd2)

```
